{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>portfolio_id</th>\n",
       "      <th>product_term_credit_limit</th>\n",
       "      <th>NP</th>\n",
       "      <th>cash_intent</th>\n",
       "      <th>ALJ0300</th>\n",
       "      <th>ALJ0316</th>\n",
       "      <th>ALJ0416</th>\n",
       "      <th>ALJ5030</th>\n",
       "      <th>ALJ5320</th>\n",
       "      <th>ALJ5730</th>\n",
       "      <th>...</th>\n",
       "      <th>TSTU0910</th>\n",
       "      <th>TSTU2906</th>\n",
       "      <th>TSTU2907</th>\n",
       "      <th>TSTU2908</th>\n",
       "      <th>TSTU3906</th>\n",
       "      <th>TSTU3907</th>\n",
       "      <th>TSTU3908</th>\n",
       "      <th>TSTU4906</th>\n",
       "      <th>TSTU4907</th>\n",
       "      <th>TSTU4908</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>record_nb</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1908</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33521</td>\n",
       "      <td>50283</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1908</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>999999998</td>\n",
       "      <td>999999998</td>\n",
       "      <td>999999998</td>\n",
       "      <td>...</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>999999998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1908</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>999999997</td>\n",
       "      <td>999999997</td>\n",
       "      <td>999999997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1908</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>999999998</td>\n",
       "      <td>999999998</td>\n",
       "      <td>999999998</td>\n",
       "      <td>...</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>999999998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1908</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>999999997</td>\n",
       "      <td>999999997</td>\n",
       "      <td>999999997</td>\n",
       "      <td>...</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>999999998.0</td>\n",
       "      <td>999999998.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2329 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           portfolio_id  product_term_credit_limit  NP  cash_intent  ALJ0300  \\\n",
       "record_nb                                                                      \n",
       "1                  1908                        500   0            0        2   \n",
       "2                  1908                        500   0            0        0   \n",
       "3                  1908                        500   0            0        1   \n",
       "4                  1908                        500   0            0        0   \n",
       "5                  1908                        500   0            1        2   \n",
       "\n",
       "           ALJ0316  ALJ0416    ALJ5030    ALJ5320    ALJ5730  ...  \\\n",
       "record_nb                                                     ...   \n",
       "1                1        1      33521      50283          0  ...   \n",
       "2               98       98  999999998  999999998  999999998  ...   \n",
       "3                0        0  999999997  999999997  999999997  ...   \n",
       "4               98       98  999999998  999999998  999999998  ...   \n",
       "5                0        0  999999997  999999997  999999997  ...   \n",
       "\n",
       "              TSTU0910  TSTU2906     TSTU2907     TSTU2908  TSTU3906  \\\n",
       "record_nb                                                              \n",
       "1                  0.0       0.0          0.0          0.0       0.0   \n",
       "2          999999998.0      98.0  999999998.0  999999998.0      98.0   \n",
       "3                  0.0       0.0          0.0          0.0       0.0   \n",
       "4          999999998.0      98.0  999999998.0  999999998.0      98.0   \n",
       "5          999999998.0      98.0  999999998.0  999999998.0      98.0   \n",
       "\n",
       "              TSTU3907     TSTU3908  TSTU4906     TSTU4907     TSTU4908  \n",
       "record_nb                                                                \n",
       "1                  0.0          0.0       0.0          0.0          0.0  \n",
       "2          999999998.0  999999998.0      98.0  999999998.0  999999998.0  \n",
       "3                  0.0          0.0       0.0          0.0          0.0  \n",
       "4          999999998.0  999999998.0      98.0  999999998.0  999999998.0  \n",
       "5          999999998.0  999999998.0      98.0  999999998.0  999999998.0  \n",
       "\n",
       "[5 rows x 2329 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fraud_risk_dataset.csv', header=0, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler, normalize, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the null values\n",
    "ccfullData = df.fillna(0)\n",
    "ccfullData.drop('NP', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the categorical values to later convert these variables using one hot encoding and keeping only the continuous numeric variables for standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_b = ccfullData.drop(['portfolio_id','product_term_credit_limit','cash_intent','ALL9950','ALL9951','GLBDECS','ALL6310','ALL6320','MTF6326'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19030, 2319)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using **RobustScaler()**, we can remove the outliers and then use either StandardScaler for preprocessing the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "scaled_x_b = scaler.fit_transform(x_b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_ccfullData = scaler.fit_transform(scaled_x_b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19030, 2319)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_df = pd.DataFrame(scaled_ccfullData)\n",
    "scaler_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables are converted to one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(sparse=False, drop= 'first', handle_unknown = 'error') \n",
    "\n",
    "onehot.fit(ccfullData.portfolio_id.to_numpy().reshape(-1,1))\n",
    "year_onehot = onehot.transform(ccfullData.portfolio_id.to_numpy().reshape(-1,1))\n",
    "year_onehot = pd.DataFrame(year_onehot, columns= onehot.get_feature_names(['portfolio_id']))\n",
    "\n",
    "onehot.fit(ccfullData.product_term_credit_limit.to_numpy().reshape(-1,1))\n",
    "product_term_onehot = onehot.transform(ccfullData.product_term_credit_limit.to_numpy().reshape(-1,1))\n",
    "product_term_onehot = pd.DataFrame(product_term_onehot, columns= onehot.get_feature_names(['product_term_credit_limit']))\n",
    "\n",
    "onehot.fit(ccfullData.cash_intent.to_numpy().reshape(-1,1))\n",
    "cash_intent_onehot = onehot.transform(ccfullData.cash_intent.to_numpy().reshape(-1,1))\n",
    "cash_intent_onehot = pd.DataFrame(cash_intent_onehot, columns= onehot.get_feature_names(['cash_intent']))\n",
    "\n",
    "#Credit bureau attributes with Flag unit\n",
    "onehot.fit(ccfullData.ALL9950.to_numpy().reshape(-1,1))\n",
    "ALL9950_onehot = onehot.transform(ccfullData.ALL9950.to_numpy().reshape(-1,1))\n",
    "ALL9950_onehot = pd.DataFrame(ALL9950_onehot, columns= onehot.get_feature_names(['ALL9950']))\n",
    "\n",
    "onehot.fit(ccfullData.ALL9951.to_numpy().reshape(-1,1))\n",
    "ALL9951_onehot = onehot.transform(ccfullData.ALL9951.to_numpy().reshape(-1,1))\n",
    "ALL9951_onehot = pd.DataFrame(ALL9951_onehot, columns= onehot.get_feature_names(['ALL9951']))\n",
    "\n",
    "onehot.fit(ccfullData.GLBDECS.to_numpy().reshape(-1,1))\n",
    "GLBDECS_onehot = onehot.transform(ccfullData.GLBDECS.to_numpy().reshape(-1,1))\n",
    "GLBDECS_onehot = pd.DataFrame(GLBDECS_onehot, columns= onehot.get_feature_names(['GLBDECS']))\n",
    "\n",
    "#Credit bureau attributes with Rank unit\n",
    "onehot.fit(ccfullData.ALL6310.to_numpy().reshape(-1,1))\n",
    "ALL6310_onehot = onehot.transform(ccfullData.ALL6310.to_numpy().reshape(-1,1))\n",
    "ALL6310_onehot = pd.DataFrame(ALL6310_onehot, columns= onehot.get_feature_names(['ALL6310']))\n",
    "\n",
    "onehot.fit(ccfullData.ALL6320.to_numpy().reshape(-1,1))\n",
    "ALL6320_onehot = onehot.transform(ccfullData.ALL6320.to_numpy().reshape(-1,1))\n",
    "ALL6320_onehot = pd.DataFrame(ALL6320_onehot, columns= onehot.get_feature_names(['ALL6320']))\n",
    "\n",
    "onehot.fit(ccfullData.MTF6326.to_numpy().reshape(-1,1))\n",
    "MTF6326_onehot = onehot.transform(ccfullData.MTF6326.to_numpy().reshape(-1,1))\n",
    "MTF6326_onehot = pd.DataFrame(MTF6326_onehot, columns= onehot.get_feature_names(['MTF6326']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_dataframes = pd.concat(\n",
    "    [\n",
    "        year_onehot.reset_index(drop=True),\n",
    "        product_term_onehot.reset_index(drop=True),\n",
    "        cash_intent_onehot.reset_index(drop=True),\n",
    "        ALL9950_onehot.reset_index(drop=True),\n",
    "        ALL9951_onehot.reset_index(drop=True),\n",
    "        GLBDECS_onehot.reset_index(drop=True),\n",
    "        ALL6310_onehot.reset_index(drop=True),\n",
    "        ALL6320_onehot.reset_index(drop=True),\n",
    "        MTF6326_onehot.reset_index(drop=True),\n",
    "        scaler_df.reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1,\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "concatenated_dataframes_columns = [\n",
    "    list(year_onehot.columns),\n",
    "    list(product_term_onehot.columns),\n",
    "    list(cash_intent_onehot.columns),\n",
    "    list(ALL9950_onehot.columns),\n",
    "    list(ALL9951_onehot.columns),\n",
    "    list(GLBDECS_onehot.columns),\n",
    "    list(ALL6310_onehot.columns),\n",
    "    list(ALL6320_onehot.columns),\n",
    "    list(MTF6326_onehot.columns),\n",
    "    list(scaler_df.columns)\n",
    "]\n",
    "    \n",
    "flatten = lambda nested_lists: [item for sublist in nested_lists for item in sublist]\n",
    "\n",
    "concatenated_dataframes.columns = flatten(concatenated_dataframes_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['NP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from statistics import mean, stdev\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score, recall_score,auc,roc_curve\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of possible accuracies for Logistic Regression is: [0.91671046 0.9224908  0.91644771 0.92170257 0.91749869]\n",
      "List of possible Precision for Logistic Regression is: [0.59333333 0.65432099 0.60629921 0.68217054 0.58659218]\n",
      "List of possible Recall for Logistic Regression is: [0.25797101 0.30724638 0.22318841 0.25507246 0.30434783]\n",
      "List of possible F1 score for Logistic Regression is: [0.35959596 0.41814596 0.32627119 0.37130802 0.40076336]\n",
      "Logistic Regression: 0.918970 (0.002588)\n",
      "---Classifier Logistic Regression use 132.3159 seconds ---\n",
      "List of possible accuracies for XGBoost is: [0.92433001 0.92485549 0.92643195 0.92143983 0.92616921]\n",
      "List of possible Precision for XGBoost is: [0.72440945 0.70921986 0.77310924 0.71698113 0.72535211]\n",
      "List of possible Recall for XGBoost is: [0.26666667 0.28985507 0.26666667 0.22028986 0.29855072]\n",
      "List of possible F1 score for XGBoost is: [0.38983051 0.41152263 0.39655172 0.33702882 0.42299795]\n",
      "XGBoost: 0.924645 (0.001785)\n",
      "---Classifier XGBoost use 1143.3989 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA used</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.9246</td>\n",
       "      <td>0.7298</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>0.391586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9190</td>\n",
       "      <td>0.6245</td>\n",
       "      <td>0.2696</td>\n",
       "      <td>0.375217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              MLA used  Test Accuracy  Precision  Recall        f1\n",
       "1              XGBoost         0.9246     0.7298  0.2684  0.391586\n",
       "0  Logistic Regression         0.9190     0.6245  0.2696  0.375217"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = concatenated_dataframes\n",
    "Y = y\n",
    "MLA_columns = []\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "row_index = 0\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression()))\n",
    "models.append(('XGBoost', XGBClassifier(eval_metric='mlogloss')))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# Create StratifiedKFold object.    \n",
    "tic = time.perf_counter()\n",
    "for name, model in models:\n",
    "    skfold = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X, Y, cv=skfold, scoring=scoring)\n",
    "    f1_results = model_selection.cross_val_score(model, X, Y, cv=skfold, scoring='f1')\n",
    "    recall_results = model_selection.cross_val_score(model, X, Y, cv=skfold, scoring='recall')\n",
    "    precision_results = model_selection.cross_val_score(model, X, Y, cv=skfold, scoring='precision')\n",
    "    MLA_compare.loc[row_index,'MLA used'] = name\n",
    "    MLA_compare.loc[row_index, 'Test Accuracy'] = round(mean(cv_results), 4)\n",
    "    MLA_compare.loc[row_index, 'Precision'] = round(mean(precision_results),4)\n",
    "    MLA_compare.loc[row_index, 'Recall'] = round(mean(recall_results),4)\n",
    "    MLA_compare.loc[row_index, 'f1'] = mean(f1_results)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    # Print the output.\n",
    "    print('List of possible accuracies for {0} is: {1}'.format(name, cv_results))\n",
    "    print('List of possible Precision for {0} is: {1}'.format(name, precision_results))\n",
    "    print('List of possible Recall for {0} is: {1}'.format(name, recall_results))\n",
    "    print('List of possible F1 score for {0} is: {1}'.format(name, f1_results))\n",
    "    msg = \"%s: %f (%f)\" % (name, mean(cv_results), cv_results.std())\n",
    "    print(msg)\n",
    "    toc = time.perf_counter()\n",
    "    secs = toc - tic\n",
    "    print(\"---Classifier %s use %0.4f seconds ---\" %(name, secs))\n",
    "    row_index+=1\n",
    "    \n",
    "MLA_compare.sort_values(by = ['Test Accuracy'], ascending = False, inplace = True)    \n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating plot to show the train accuracy\n",
    "plt.subplots(figsize=(5,5))\n",
    "sns.barplot(x=\"MLA used\", y=\"Test Accuracy\",data=MLA_compare,palette='Set2',edgecolor=sns.color_palette('Accent',7))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('MLA Test Accuracy Comparison')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
